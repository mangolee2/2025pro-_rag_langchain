{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdea0d98",
   "metadata": {},
   "source": [
    "### NER-augmented RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df1aef",
   "metadata": {},
   "source": [
    "ollama llm ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a0cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0.5: OllamaLLM í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 0.5: OllamaLLM ì •ì˜\n",
    "# ==========================\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"gpt-oss\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    timeout: int = 300  # ëª¨ë¸ í¬ê¸° ë•Œë¬¸ì— ì¶©ë¶„íˆ ë„‰ë„‰íˆ ì„¤ì •\n",
    "    \n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except requests.Timeout:\n",
    "            raise RuntimeError(f\"Ollama timed out after {self.timeout}s\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {str(e)}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "print(\"Step 0.5: OllamaLLM í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678cbe2",
   "metadata": {},
   "source": [
    "PDF â†’ RAG â†’ BioNER â†’ í›„ì²˜ë¦¬ â†’ ìµœì¢… drug list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32453958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "# ==========================\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "\n",
    "# ==========================\n",
    "# Step 2: PDF ë¡œë“œ\n",
    "# ==========================\n",
    "pdf_path = \"/data1/workspace/pdfs/13.pdf\"  # PDF ê²½ë¡œ\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# ==========================\n",
    "# Step 3: í…ìŠ¤íŠ¸ Chunk ë¶„í• \n",
    "# ==========================\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 4: ì„ë² ë”© + FAISS ë²¡í„°ìŠ¤í† ì–´\n",
    "# ==========================\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f137967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 5: BioNER ëª¨ë¸ ë¡œë“œ\n",
    "# ==========================\n",
    "ner_model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    ner_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 6: NER ì¶”ì¶œ + WordPiece í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ==========================\n",
    "def extract_drugs(text, ner_pipe, min_score=0.9):\n",
    "    ner_results = ner_pipe(text)\n",
    "    # WordPiece í•©ì¹˜ê¸°\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication' and token['score'] >= min_score:\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    drugs = list(set(drugs))\n",
    "    return drugs\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_text = split_documents[0].page_content\n",
    "drugs_in_chunk = extract_drugs(test_text, ner_pipe)\n",
    "# print(\"Step 6: ì²« ë²ˆì§¸ Chunkì—ì„œ ì¶”ì¶œëœ drug names:\", drugs_in_chunk)\n",
    "\n",
    "# ==========================\n",
    "# Step 6: NER ì¶”ì¶œ + WordPiece í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ==========================\n",
    "def extract_drugs(text, ner_pipe, min_score=0.9):\n",
    "    ner_results = ner_pipe(text)\n",
    "    # WordPiece í•©ì¹˜ê¸°\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication' and token['score'] >= min_score:\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    drugs = list(set(drugs))\n",
    "    return drugs\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_text = split_documents[0].page_content\n",
    "drugs_in_chunk = extract_drugs(test_text, ner_pipe)\n",
    "# print(\"Step 6: ì²« ë²ˆì§¸ Chunkì—ì„œ ì¶”ì¶œëœ drug names:\", drugs_in_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d8be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/tmp/ipykernel_3373457/1871298209.py:66: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain({\"query\": all_text})\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: RetrievalQA ì²´ì¸ ìƒì„± ì™„ë£Œ\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Step 8: RAG + LLMë¡œ ì¶”ì¶œëœ Drug Names:\n",
      " BRM270\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 7: ì „ì²´ PDFì—ì„œ Drug ì¶”ì¶œ\n",
    "# ==========================\n",
    "all_drugs = []\n",
    "for i, chunk in enumerate(split_documents):\n",
    "    chunk_drugs = extract_drugs(chunk.page_content, ner_pipe)\n",
    "    all_drugs.extend(chunk_drugs)\n",
    "all_drugs = list(set(all_drugs))\n",
    "\n",
    "# ==========================\n",
    "# Step 8: RAG + OllamaLLM ì—°ë™ (ì •ë¦¬)\n",
    "# ==========================\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# LLM ê°ì²´ ìƒì„±\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\", temperature=0)\n",
    "# print(\"Step 8: OllamaLLM ê°ì²´ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# Prompt ì •ì˜\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical text analysis assistant.\n",
    "\n",
    "Your goal is to extract **only the drugs that were experimentally tested, administered, or directly used in the study**.\n",
    "Focus on precision â€” if uncertain, do not include the drug.\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "Guidelines:\n",
    "- Prefer drugs explicitly described as being *tested*, *treated*, *administered*, or *used* in the experiments.\n",
    "- Exclude drugs mentioned only in background, discussion, or references.\n",
    "- Ignore drugs that appear as examples, related compounds, or comparative mentions unless they were actually used.\n",
    "- Merge WordPiece fragments into full drug names.\n",
    "- Remove duplicates.\n",
    "- Extract **at least 1** and **at most 3** drug names.\n",
    "- If no clear experimental drugs are found, return \"None\".\n",
    "- Output only the extracted drug names, separated by semicolons (;), with no extra text or explanation.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "# RetrievalQA ì²´ì¸ ìƒì„±\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"Step 8: RetrievalQA ì²´ì¸ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# PDF ì „ì²´ text í•©ì¹˜ê¸°\n",
    "all_text = \" \".join([chunk.page_content for chunk in split_documents])\n",
    "# print(\"Step 8: ëª¨ë“  chunk í•©ì¹˜ê¸° ì™„ë£Œ, ê¸¸ì´:\", len(all_text))\n",
    "\n",
    "# RAG + LLM ì‹¤í–‰\n",
    "response = chain({\"query\": all_text})\n",
    "rag_drugs = response['result']\n",
    "print(\"Step 8: RAG + LLMë¡œ ì¶”ì¶œëœ Drug Names:\\n\", rag_drugs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89bdc9",
   "metadata": {},
   "source": [
    "fda ìŠ¹ì¸ ì•½ë¬¼ í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0adbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Step 9: LLMì´ ì¶”ì¶œí•œ í›„ë³´: ['brm270']\n",
      "ğŸ”¹ Step 9: FDA ìŠ¹ì¸ ì•½ë¬¼ í•„í„°ë§ ê²°ê³¼: []\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 9: FDA ìŠ¹ì¸ í•­ì•”ì œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… 1ï¸âƒ£: FDA ìŠ¹ì¸ í•­ì•”ì œ ëª©ë¡ (ê°„ëµ ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ)\n",
    "# ì‹¤ì œë¡œëŠ” DrugBank, NCI, FDA ê³µì‹ DBì—ì„œ csvë¡œ ë°›ëŠ” ê²Œ ì •í™•í•˜ì§€ë§Œ,\n",
    "# ì—¬ê¸°ì„  ì‹œì—°ìš©ìœ¼ë¡œ ì£¼ìš” í•­ì•”ì œ 100ì—¬ê°œë§Œ ì˜ˆì‹œë¡œ ë‘ .\n",
    "approved_anticancer_drugs = [\n",
    "    \"cisplatin\", \"carboplatin\", \"oxaliplatin\",\n",
    "    \"paclitaxel\", \"docetaxel\", \"vincristine\", \"vinblastine\",\n",
    "    \"doxorubicin\", \"epirubicin\", \"cyclophosphamide\",\n",
    "    \"ifosfamide\", \"etoposide\", \"irinotecan\", \"topotecan\",\n",
    "    \"gemcitabine\", \"fluorouracil\", \"capecitabine\",\n",
    "    \"methotrexate\", \"pemetrexed\", \"temozolomide\",\n",
    "    \"sunitinib\", \"sorafenib\", \"lenvatinib\", \"regorafenib\",\n",
    "    \"bevacizumab\", \"cetuximab\", \"panitumumab\",\n",
    "    \"trastuzumab\", \"pertuzumab\", \"ado-trastuzumab emtansine\",\n",
    "    \"nivolumab\", \"pembrolizumab\", \"atezolizumab\",\n",
    "    \"durvalumab\", \"avelumab\", \"ipilimumab\",\n",
    "    \"palbociclib\", \"ribociclib\", \"abemaciclib\",\n",
    "    \"afatinib\", \"gefitinib\", \"erlotinib\", \"osimertinib\",\n",
    "    \"crizotinib\", \"ceritinib\", \"lorlatinib\",\n",
    "    \"dabrafenib\", \"trametinib\", \"vemurafenib\", \"encorafenib\",\n",
    "    \"everolimus\", \"temsirolimus\",\n",
    "    \"olaparib\", \"rucaparib\", \"niraparib\", \"talazoparib\",\n",
    "    \"veliparib\",\n",
    "    \"bortezomib\", \"ixazomib\", \"carfilzomib\",\n",
    "    \"lenalidomide\", \"thalidomide\", \"pomalidomide\",\n",
    "    \"venetoclax\", \"idelalisib\", \"copanlisib\",\n",
    "    \"tucatinib\", \"alpelisib\", \"larotrectinib\",\n",
    "    \"entrectinib\", \"selpercatinib\", \"pralsetinib\",\n",
    "    \"imatinib\", \"dasatinib\", \"nilotinib\", \"bosutinib\", \"ponatinib\",\n",
    "    \"cabozantinib\", \"vandetanib\", \"axitinib\", \"pazopanib\",\n",
    "    \"sorafenib\", \"lenvatinib\", \"everolimus\",\n",
    "]\n",
    "\n",
    "# âœ… 2ï¸âƒ£: RAG ê²°ê³¼ë¥¼ ì†Œë¬¸ì ë³€í™˜í•˜ê³  í•„í„°ë§\n",
    "rag_drug_candidates = [d.strip().lower() for d in rag_drugs.split(\";\")]\n",
    "filtered_drugs = [\n",
    "    d for d in rag_drug_candidates if d in approved_anticancer_drugs\n",
    "]\n",
    "\n",
    "# âœ… 3ï¸âƒ£: ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ”¹ Step 9: LLMì´ ì¶”ì¶œí•œ í›„ë³´:\", rag_drug_candidates)\n",
    "print(\"ğŸ”¹ Step 9: FDA ìŠ¹ì¸ ì•½ë¬¼ í•„í„°ë§ ê²°ê³¼:\", filtered_drugs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b41ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
