{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3401aa6",
   "metadata": {},
   "source": [
    "RAG-LangChain llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "36f3f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"/data1/workspace/pdfs/32.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"  # 기본 Ollama 서버 주소\n",
    ")\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "23ca39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "import subprocess\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"llama3.2-vision\"\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        result = subprocess.run(\n",
    "            ['ollama', 'run', self.model_name, prompt],\n",
    "            capture_output=True, \n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Ollama LLM failed: {result.stderr}\")\n",
    "        return result.stdout.strip()\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model_name\": self.model_name, \"temperature\": self.temperature}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ollama\"\n",
    "    \n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = OllamaLLM(model_name=\"llama3.2-vision\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"  # 또는 \"map_reduce\", \"refine\", \"map_rerank\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9bebbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, List, Any\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"llama3.2-vision\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    timeout: int = 300  # 13GB 모델이라 넉넉히 설정\n",
    "    \n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except requests.Timeout:\n",
    "            raise RuntimeError(f\"Ollama timed out after {self.timeout}s\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {str(e)}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "# 사용\n",
    "llm = OllamaLLM(model_name=\"llama3.2-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f1a81c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+2 = 4\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "답이없음\n"
     ]
    }
   ],
   "source": [
    "# 1. LLM 직접 테스트\n",
    "llm = OllamaLLM(model_name=\"llama3.2-vision\")\n",
    "print(llm(\"What is 2+2?\"))\n",
    "\n",
    "# 2. 체인 실행\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = chain({\"query\": \"논문에서 주된 신약명은?\"})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ae605401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "irinotecan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 커스텀 프롬프트 정의\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical text analysis assistant.\n",
    "\n",
    "Your task is to extract **exactly one** experimental drug name that was actually tested or administered in the following document excerpt.\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "Instructions:\n",
    "- Extract **exactly one** drug name — not two, not three, only one.\n",
    "- Choose the **single most likely experimental drug** based on the context of treatment, administration, or dosage.\n",
    "- If multiple drugs are mentioned, select the **main or most central one** (e.g., the primary treatment or intervention).\n",
    "- Exclude drugs mentioned only in background, literature review, or control conditions.\n",
    "- Do not include vehicles, buffers, or solutions (e.g., saline, DMSO, PBS).\n",
    "- Merge WordPiece fragments into full drug names.\n",
    "- If uncertain, make your **best single guess** rather than listing multiple possibilities.\n",
    "- Output **only the drug name** — no lists, no punctuation, no explanations.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# RetrievalQA에 적용\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 실행\n",
    "question = \"Extract generic drug names from this paper\"\n",
    "response = chain({\"query\": question})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e6f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
