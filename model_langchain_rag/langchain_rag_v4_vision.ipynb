{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3401aa6",
   "metadata": {},
   "source": [
    "RAG-LangChain Multi-modal LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa1ab5",
   "metadata": {},
   "source": [
    "PDF 로드 & 이미지 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800b83d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Loading PDF: /data1/workspace/pdfs/1.pdf\n",
      "  → 총 16 페이지\n",
      "  - Page 1: 4 images\n",
      "    → Saved: /data1/workspace/pdf_images/page1_1.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page1_2.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page1_3.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page1_4.png\n",
      "  - Page 2: 0 images\n",
      "  - Page 3: 0 images\n",
      "  - Page 4: 0 images\n",
      "  - Page 5: 1 images\n",
      "    → Saved: /data1/workspace/pdf_images/page5_1.jpeg\n",
      "  - Page 6: 9 images\n",
      "    → Saved: /data1/workspace/pdf_images/page6_1.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_2.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_3.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_4.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_5.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_6.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_7.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_8.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page6_9.jpeg\n",
      "  - Page 7: 0 images\n",
      "  - Page 8: 1 images\n",
      "    → Saved: /data1/workspace/pdf_images/page8_1.jpeg\n",
      "  - Page 9: 0 images\n",
      "  - Page 10: 1 images\n",
      "    → Saved: /data1/workspace/pdf_images/page10_1.jpeg\n",
      "  - Page 11: 0 images\n",
      "  - Page 12: 10 images\n",
      "    → Saved: /data1/workspace/pdf_images/page12_1.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_2.jpeg\n",
      "    → Saved: /data1/workspace/pdf_images/page12_3.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_4.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_5.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_6.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_7.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_8.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_9.png\n",
      "    → Saved: /data1/workspace/pdf_images/page12_10.png\n",
      "  - Page 13: 0 images\n",
      "  - Page 14: 1 images\n",
      "    → Saved: /data1/workspace/pdf_images/page14_1.jpeg\n",
      "  - Page 15: 0 images\n",
      "  - Page 16: 0 images\n",
      "✅ 이미지 추출 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "pdf_path = \"/data1/workspace/pdfs/1.pdf\"\n",
    "img_dir = \"/data1/workspace/pdf_images\"\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "print(f\"[1] Loading PDF: {pdf_path}\")\n",
    "doc = fitz.open(pdf_path)\n",
    "print(f\"  → 총 {len(doc)} 페이지\")\n",
    "\n",
    "for page_index in range(len(doc)):\n",
    "    page = doc.load_page(page_index)\n",
    "    images = page.get_images(full=True)\n",
    "    print(f\"  - Page {page_index+1}: {len(images)} images\")\n",
    "    for i, img in enumerate(images):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        img_bytes = base_image[\"image\"]\n",
    "        img_ext = base_image[\"ext\"]\n",
    "        img_path = os.path.join(img_dir, f\"page{page_index+1}_{i+1}.{img_ext}\")\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(img_bytes)\n",
    "        print(f\"    → Saved: {img_path}\")\n",
    "\n",
    "print(\"✅ 이미지 추출 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649ab19",
   "metadata": {},
   "source": [
    "[STEP 2] Vision 모델 테스트 (LLaVA or Llama3.2-Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985cd724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 이미지: /data1/workspace/pdf_images/page10_1.jpeg\n",
      "[2] Vision 모델로 이미지 분석: llava\n",
      "  → 결과 요약 (앞부분 300자):\n",
      " The image is a composite of several photos and text, which appears to be an informational or educational collage. The central part shows a microscope slide with immunohistochemistry-stained sections that are likely from tissue samples. These sections have antibodies directed against specific protei...\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "VISION_MODEL = \"llava\"  # 또는 \"llama3.2-vision\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def analyze_image_with_ollama(image_path, prompt=\"Describe this figure.\"):\n",
    "    print(f\"[2] Vision 모델로 이미지 분석: {VISION_MODEL}\")\n",
    "    try:\n",
    "        # ✅ 이미지 base64 인코딩\n",
    "        with open(image_path, \"rb\") as img:\n",
    "            image_b64 = base64.b64encode(img.read()).decode(\"utf-8\")\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": VISION_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_b64],\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(OLLAMA_URL, json=payload, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json().get(\"response\", \"\")\n",
    "        print(f\"  → 결과 요약 (앞부분 300자):\\n{result[:300]}...\")\n",
    "        return result\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"⚠️ HTTP 오류: {e.response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Vision 모델 실행 오류: {e}\")\n",
    "\n",
    "# 첫 이미지로 다시 테스트\n",
    "import os\n",
    "images = sorted([f for f in os.listdir(img_dir) if f.lower().endswith((\"png\",\"jpg\",\"jpeg\"))])\n",
    "if images:\n",
    "    sample = os.path.join(img_dir, images[0])\n",
    "    print(\"테스트 이미지:\", sample)\n",
    "    analyze_image_with_ollama(sample)\n",
    "else:\n",
    "    print(\"⚠️ 추출된 이미지가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef3c1b",
   "metadata": {},
   "source": [
    "[STEP 3] 텍스트 로드 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2be3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] 텍스트 추출 시작...\n",
      "  → 로드된 문서 수: 16\n",
      "  → 분할된 청크 수: 186\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"[3] 텍스트 추출 시작...\")\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "print(f\"  → 로드된 문서 수: {len(docs)}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "print(f\"  → 분할된 청크 수: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f763dd",
   "metadata": {},
   "source": [
    "[STEP 4] 임베딩 & 벡터스토어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69472c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] 벡터스토어 생성 중...\n",
      "✅ 벡터스토어 구축 완료\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "print(\"[4] 벡터스토어 생성 중...\")\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"✅ 벡터스토어 구축 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95761a1",
   "metadata": {},
   "source": [
    "[STEP 5] LLM 연결 (Runnable 호환 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60acec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Runnable (최종버전) 등록 완료\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "import requests\n",
    "\n",
    "class OllamaRunnable(Runnable):\n",
    "    def __init__(self, model=\"gpt-oss\", base_url=\"http://localhost:11434\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def invoke(self, input, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        LangChain이 전달하는 input이 StringPromptValue, dict, str 등일 수 있음.\n",
    "        모두 안전하게 문자열로 변환 후 Ollama로 요청.\n",
    "        \"\"\"\n",
    "        # ✅ 1. LangChain PromptValue 타입 안전 처리\n",
    "        try:\n",
    "            if hasattr(input, \"to_string\"):\n",
    "                prompt_text = input.to_string()\n",
    "            elif isinstance(input, dict) and \"prompt\" in input:\n",
    "                prompt_text = input[\"prompt\"]\n",
    "            elif isinstance(input, str):\n",
    "                prompt_text = input\n",
    "            else:\n",
    "                prompt_text = str(input)\n",
    "        except Exception:\n",
    "            prompt_text = str(input)\n",
    "\n",
    "        # ✅ 2. Ollama API 요청\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\"model\": self.model, \"prompt\": prompt_text, \"stream\": False},\n",
    "                timeout=300\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json().get(\"response\", \"\")\n",
    "            print(f\"[LLM 응답 앞부분]\\n{result[:200]}...\\n\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ LLM 오류: {e}\")\n",
    "            return f\"[LLM 오류]: {e}\"\n",
    "\n",
    "llm = OllamaRunnable(model=\"gpt-oss\")\n",
    "print(\"✅ LLM Runnable (최종버전) 등록 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7aef6",
   "metadata": {},
   "source": [
    "[STEP 6] RetrievalQA 체인 구성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e853e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] RAG 체인 구성 중...\n",
      "✅ 체인 생성 완료\n",
      "[LLM 응답 앞부분]\n",
      "sotorasib; adavosertib; osimertinib...\n",
      "\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[결과]\n",
      "sotorasib; adavosertib; osimertinib\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical text analysis assistant.\n",
    "\n",
    "Your task is to extract **only the drugs that were actually tested, administered, or part of the experiments** in the provided document chunks.\n",
    "Do not include drugs that are mentioned only in the background, discussion, references, or comparison sections.\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "Guidelines:\n",
    "- Extract drugs mentioned in the 'Results' or 'Methods' sections preferably.\n",
    "- Include drugs mentioned in Figures and Tables only if they were used in the main experiments.\n",
    "- Exclude drugs mentioned only as examples, background information, or in cited papers.\n",
    "- Exclude gene names, proteins, signaling pathways, or assay reagents (e.g., DMSO, PBS, MTT).\n",
    "- If the text describes a combination therapy, include all drugs that were co-administered.\n",
    "- Merge WordPiece fragments into complete drug names.\n",
    "- Remove duplicates.\n",
    "- List up to **3 most relevant drugs** directly used in experiments.\n",
    "- Verify that each extracted term is an **actual drug name or formulation**, not a biological target or herbal component.\n",
    "- Output as a **semicolon-separated list**, without any extra words or explanations.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"[6] RAG 체인 구성 중...\")\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"✅ 체인 생성 완료\")\n",
    "\n",
    "query = \"List all drugs mentioned in the results section.\"\n",
    "response = chain.invoke({\"query\": query})\n",
    "print(\"\\n[결과]\")\n",
    "print(response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a5f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
