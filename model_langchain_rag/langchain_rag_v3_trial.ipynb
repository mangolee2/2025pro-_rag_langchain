{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42440129",
   "metadata": {},
   "source": [
    "실제 파이프라인 예시 (precision 지향형)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea0d98",
   "metadata": {},
   "source": [
    "### NER-augmented RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210dcac",
   "metadata": {},
   "source": [
    "accelerate로 모델 로드 : device=0 없이(오류남), 큰 모델 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ddadb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'Medication', 'score': 0.91673374, 'word': 'lapatinib', 'start': 0, 'end': 9}, {'entity_group': 'Medication', 'score': 0.99988055, 'word': 'tr', 'start': 14, 'end': 16}, {'entity_group': 'Medication', 'score': 0.9977405, 'word': '##ast', 'start': 16, 'end': 19}, {'entity_group': 'Medication', 'score': 0.89958596, 'word': '##uzumab', 'start': 19, 'end': 25}, {'entity_group': 'Biological_structure', 'score': 0.9155983, 'word': 'breast', 'start': 38, 'end': 44}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# NER pipeline\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"Lapatinib and trastuzumab are used in breast cancer treatment.\"\n",
    "result = ner_pipe(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3ae38",
   "metadata": {},
   "source": [
    "Pipeline 생성 예제 (BioNER + RAG 연동용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09aa039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lapatinib', 'trastuzumab']\n"
     ]
    }
   ],
   "source": [
    "def merge_wordpieces(ner_results):\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication':\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    return drugs\n",
    "\n",
    "merged_drugs = merge_wordpieces(result)\n",
    "print(merged_drugs)  # ['lapatinib', 'trastuzumab']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e7c8f",
   "metadata": {},
   "source": [
    "Pipeline 생성 예제 (BioNER + RAG 연동용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bbfe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 라이브러리 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 1: 라이브러리 로드\n",
    "# ==========================\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "print(\"Step 1: 라이브러리 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99448a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: PDF 로드 완료, 문서 수: 9\n",
      "문서 내용 일부 확인:\n",
      " ORIGINAL ARTICLE\n",
      "HER2 exon 20 insertions in non-small-cell lung cancer\n",
      "are sensitive to the irreversible pan-HER receptor\n",
      "tyrosine kinase inhibitor pyrotinib\n",
      "Y. Wang1†, T. Jiang1†, Z. Qin2†, J. Jiang3, Q. Wang3, S. Yang1, C. Rivard4, G. Gao1, T. L. Ng4, M. M. Tu5,6,\n",
      "H. Yu4, H. Ji1,2‡, C. Zhou1‡, S. Ren1,4*‡, J. Zhang7, P. Bunn4, R. C. Doebele4, D. R. Camidge4 & F. R. Hirsch4\n",
      "1Department of Medical Oncology, Shanghai Pulmonary Hospital, Tongji University School of Medicine, Shanghai; 2Institute o\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 2: PDF 문서 로드\n",
    "# ==========================\n",
    "pdf_path = \"/data1/workspace/pdfs/4.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "print(f\"Step 2: PDF 로드 완료, 문서 수: {len(docs)}\")\n",
    "print(\"문서 내용 일부 확인:\\n\", docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285d325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: 문서 분할 완료, Chunk 수: 90\n",
      "첫 번째 Chunk 내용:\n",
      " ORIGINAL ARTICLE\n",
      "HER2 exon 20 insertions in non-small-cell lung cancer\n",
      "are sensitive to the irreversible pan-HER receptor\n",
      "tyrosine kinase inhibitor pyrotinib\n",
      "Y. Wang1†, T. Jiang1†, Z. Qin2†, J. Jiang3, Q. Wang3, S. Yang1, C. Rivard4, G. Gao1, T. L. Ng4, M. M. Tu5,6,\n",
      "H. Yu4, H. Ji1,2‡, C. Zhou1‡, S. Ren1,4*‡, J. Zhang7, P. Bunn4, R. C. Doebele4, D. R. Camidge4 & F. R. Hirsch4\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 3: 텍스트 Chunk 분할\n",
    "# ==========================\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Step 3: 문서 분할 완료, Chunk 수: {len(split_documents)}\")\n",
    "print(\"첫 번째 Chunk 내용:\\n\", split_documents[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752f3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: 벡터스토어 및 retriever 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 4: 임베딩 생성 및 FAISS 벡터스토어\n",
    "# ==========================\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Step 4: 벡터스토어 및 retriever 생성 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e3ef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: BioNER pipeline 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 5: BioNER 모델 로드\n",
    "# ==========================\n",
    "ner_model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    ner_model_name,\n",
    "    device_map=\"auto\"  # GPU 자동 배치\n",
    ")\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "print(\"Step 5: BioNER pipeline 생성 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29074ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: NER 결과 (raw)\n",
      " [{'entity_group': 'Medication', 'score': 0.91673374, 'word': 'lapatinib', 'start': 0, 'end': 9}, {'entity_group': 'Medication', 'score': 0.99988055, 'word': 'tr', 'start': 14, 'end': 16}, {'entity_group': 'Medication', 'score': 0.9977405, 'word': '##ast', 'start': 16, 'end': 19}, {'entity_group': 'Medication', 'score': 0.89958596, 'word': '##uzumab', 'start': 19, 'end': 25}, {'entity_group': 'Biological_structure', 'score': 0.9155983, 'word': 'breast', 'start': 38, 'end': 44}]\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 6: NER 테스트\n",
    "# ==========================\n",
    "test_text = \"Lapatinib and trastuzumab are used in breast cancer treatment.\"\n",
    "ner_result = ner_pipe(test_text)\n",
    "print(\"Step 6: NER 결과 (raw)\\n\", ner_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528f7262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: 최종 Drug Names\n",
      " ['lapatinib', 'trastuzumab']\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 7: WordPiece 후처리\n",
    "# ==========================\n",
    "def merge_wordpieces(ner_results):\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication':\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    return drugs\n",
    "\n",
    "merged_drugs = merge_wordpieces(ner_result)\n",
    "print(\"Step 7: 최종 Drug Names\\n\", merged_drugs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a0cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0.5: OllamaLLM 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 0.5: OllamaLLM 정의\n",
    "# ==========================\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"gpt-oss\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    timeout: int = 300  # 모델 크기 때문에 충분히 넉넉히 설정\n",
    "    \n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except requests.Timeout:\n",
    "            raise RuntimeError(f\"Ollama timed out after {self.timeout}s\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {str(e)}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "print(\"Step 0.5: OllamaLLM 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceaaad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: LLM 프롬프트 예시\n",
      " \n",
      "You are a biomedical text analysis assistant.\n",
      "List all unique drug names found in the document chunk below:\n",
      "\n",
      "==== Document Excerpt Start ====\n",
      "ORIGINAL ARTICLE\n",
      "HER2 exon 20 insertions in non-small-cell lung cancer\n",
      "are sensitive to the irreversible pan-HER receptor\n",
      "tyrosine kinase inhibitor pyrotinib\n",
      "Y. Wang1†, T. Jiang1†, Z. Qin2†, J. Jiang3, Q. Wang3, S. Yang1, C. Rivard4, G. Gao1, T. L. Ng4, M. M. Tu5,6,\n",
      "H. Yu4, H. Ji1,2‡, C. Zhou1‡, S. Ren1,4*‡, J. Zhang7, P. Bunn4, R. C. Doebele4, D. R. Camidge4 & F. R. Hirsch4\n",
      "==== Document Excerpt End ====\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 8 (선택): RAG + LLM 연동 예제\n",
    "# ==========================\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI  # 필요시 OllamaLLM도 사용 가능\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical text analysis assistant.\n",
    "List all unique drug names found in the document chunk below:\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "# 예시: 첫 번째 chunk에서 LLM에게 확인\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\", temperature=0)\n",
    "context = split_documents[0].page_content\n",
    "query_prompt = PROMPT.format(context=context)\n",
    "print(\"Step 8: LLM 프롬프트 예시\\n\", query_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678cbe2",
   "metadata": {},
   "source": [
    "PDF → RAG → BioNER → 후처리 → 최종 drug list → 정답셋 대비 precision 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fecbbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0.5: OllamaLLM 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 0.5: OllamaLLM 정의\n",
    "# ==========================\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"gpt-oss\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    timeout: int = 300\n",
    "    \n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except requests.Timeout:\n",
    "            raise RuntimeError(f\"Ollama timed out after {self.timeout}s\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {str(e)}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "print(\"Step 0.5: OllamaLLM 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32453958",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path /data1/workspace/pdfs/1058.full.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Step 2: PDF 로드\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[1;32m     14\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data1/workspace/pdfs/1058.full.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# PDF 경로\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyMuPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Step 3: 텍스트 Chunk 분할\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag_env/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:429\u001b[0m, in \u001b[0;36mPyMuPDFLoader.__init__\u001b[0;34m(self, file_path, headers, extract_images, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PyMuPDF` package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pymupdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_images \u001b[38;5;241m=\u001b[39m extract_images\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/rag_env/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:117\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path /data1/workspace/pdfs/1058.full.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 1: 라이브러리 로드\n",
    "# ==========================\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "\n",
    "# ==========================\n",
    "# Step 2: PDF 로드\n",
    "# ==========================\n",
    "pdf_path = \"/data1/workspace/pdfs/1058.full.pdf\"  # PDF 경로\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# ==========================\n",
    "# Step 3: 텍스트 Chunk 분할\n",
    "# ==========================\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 4: 임베딩 + FAISS 벡터스토어\n",
    "# ==========================\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f137967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 5: BioNER 모델 로드\n",
    "# ==========================\n",
    "ner_model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    ner_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 6: NER 추출 + WordPiece 후처리 함수\n",
    "# ==========================\n",
    "def extract_drugs(text, ner_pipe, min_score=0.9):\n",
    "    ner_results = ner_pipe(text)\n",
    "    # WordPiece 합치기\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication' and token['score'] >= min_score:\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    # 중복 제거\n",
    "    drugs = list(set(drugs))\n",
    "    return drugs\n",
    "\n",
    "# 테스트\n",
    "test_text = split_documents[0].page_content\n",
    "drugs_in_chunk = extract_drugs(test_text, ner_pipe)\n",
    "# print(\"Step 6: 첫 번째 Chunk에서 추출된 drug names:\", drugs_in_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ab16f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Step 6: NER 추출 + WordPiece 후처리 함수\n",
    "# ==========================\n",
    "def extract_drugs(text, ner_pipe, min_score=0.9):\n",
    "    ner_results = ner_pipe(text)\n",
    "    # WordPiece 합치기\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication' and token['score'] >= min_score:\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    # 중복 제거\n",
    "    drugs = list(set(drugs))\n",
    "    return drugs\n",
    "\n",
    "# 테스트\n",
    "test_text = split_documents[0].page_content\n",
    "drugs_in_chunk = extract_drugs(test_text, ner_pipe)\n",
    "# print(\"Step 6: 첫 번째 Chunk에서 추출된 drug names:\", drugs_in_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d8be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/tmp/ipykernel_3372472/39056141.py:58: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain({\"query\": all_text})\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: OllamaLLM 객체 생성 완료\n",
      "Step 8: RetrievalQA 체인 생성 완료\n",
      "Step 8: 모든 chunk 합치기 완료, 길이: 40178\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Step 8: RAG + LLM로 추출된 Drug Names:\n",
      " AP32788; TAK-788; neratinib; afatinib; pyrotinib; PF299804; HKI-272\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 7: 전체 PDF에서 Drug 추출\n",
    "# ==========================\n",
    "all_drugs = []\n",
    "for i, chunk in enumerate(split_documents):\n",
    "    chunk_drugs = extract_drugs(chunk.page_content, ner_pipe)\n",
    "    all_drugs.extend(chunk_drugs)\n",
    "all_drugs = list(set(all_drugs))\n",
    "\n",
    "# ==========================\n",
    "# Step 8: RAG + OllamaLLM 연동 (정리)\n",
    "# ==========================\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# LLM 객체 생성\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\", temperature=0)\n",
    "print(\"Step 8: OllamaLLM 객체 생성 완료\")\n",
    "\n",
    "# Prompt 정의\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical text analysis assistant.\n",
    "\n",
    "Extract and clean all unique drug names from the following document chunks:\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "- Merge broken tokens (WordPiece) into full drug names\n",
    "- Remove duplicates\n",
    "- Output as a semicolon-separated list\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "# RetrievalQA 체인 생성\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"Step 8: RetrievalQA 체인 생성 완료\")\n",
    "\n",
    "# PDF 전체 text 합치기\n",
    "all_text = \" \".join([chunk.page_content for chunk in split_documents])\n",
    "print(\"Step 8: 모든 chunk 합치기 완료, 길이:\", len(all_text))\n",
    "\n",
    "# RAG + LLM 실행\n",
    "response = chain({\"query\": all_text})\n",
    "rag_drugs = response['result']\n",
    "print(\"Step 8: RAG + LLM로 추출된 Drug Names:\\n\", rag_drugs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d6093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
