{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdea0d98",
   "metadata": {},
   "source": [
    "### NER-augmented RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df1aef",
   "metadata": {},
   "source": [
    "ollama llm 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a0cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"gpt-oss\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    timeout: int = 300\n",
    "    \n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\"model\": self.model_name, \"prompt\": prompt, \"stream\": False}\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {str(e)}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678cbe2",
   "metadata": {},
   "source": [
    "PDF → RAG → BioNER → 후처리 → 최종 drug list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32453958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 1: 라이브러리 로드\n",
    "# ==========================\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "\n",
    "# ==========================\n",
    "# Step 2: PDF 로드\n",
    "# ==========================\n",
    "pdf_path = \"/data1/workspace/pdfs/4.pdf\"  # PDF 경로\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# ==========================\n",
    "# Step 3: 텍스트 Chunk 분할\n",
    "# ==========================\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 4: 임베딩 + FAISS 벡터스토어\n",
    "# ==========================\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f137967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 5: BioNER 모델 로드\n",
    "# ==========================\n",
    "ner_model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    ner_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 6: NER 추출 + WordPiece 후처리 함수\n",
    "# ==========================\n",
    "def extract_drugs(text, ner_pipe, min_score=0.9):\n",
    "    ner_results = ner_pipe(text)\n",
    "    # WordPiece 합치기\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication' and token['score'] >= min_score:\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    # 중복 제거\n",
    "    drugs = list(set(drugs))\n",
    "    return drugs\n",
    "\n",
    "# 테스트\n",
    "test_text = split_documents[0].page_content\n",
    "drugs_in_chunk = extract_drugs(test_text, ner_pipe)\n",
    "# print(\"Step 6: 첫 번째 Chunk에서 추출된 drug names:\", drugs_in_chunk)\n",
    "\n",
    "# ==========================\n",
    "# Step 6: NER 추출 + WordPiece 후처리 함수\n",
    "# ==========================\n",
    "def extract_drugs(text, ner_pipe, min_score=0.9):\n",
    "    ner_results = ner_pipe(text)\n",
    "    # WordPiece 합치기\n",
    "    drugs = []\n",
    "    current_word = \"\"\n",
    "    for token in ner_results:\n",
    "        if token['entity_group'] == 'Medication' and token['score'] >= min_score:\n",
    "            if token['word'].startswith(\"##\"):\n",
    "                current_word += token['word'][2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    drugs.append(current_word)\n",
    "                current_word = token['word']\n",
    "    if current_word:\n",
    "        drugs.append(current_word)\n",
    "    # 중복 제거\n",
    "    drugs = list(set(drugs))\n",
    "    return drugs\n",
    "\n",
    "# 테스트\n",
    "test_text = split_documents[0].page_content\n",
    "drugs_in_chunk = extract_drugs(test_text, ner_pipe)\n",
    "# print(\"Step 6: 첫 번째 Chunk에서 추출된 drug names:\", drugs_in_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d8be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/tmp/ipykernel_3372278/2742292314.py:59: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain({\"query\": all_text})\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Step 8: RAG + LLM로 추출된 Drug Names:\n",
      " pyrotinib; neratinib; TAK-788\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 7: 전체 PDF에서 Drug 추출\n",
    "# ==========================\n",
    "all_drugs = []\n",
    "for i, chunk in enumerate(split_documents):\n",
    "    chunk_drugs = extract_drugs(chunk.page_content, ner_pipe)\n",
    "    all_drugs.extend(chunk_drugs)\n",
    "all_drugs = list(set(all_drugs))\n",
    "\n",
    "# ==========================\n",
    "# Step 8: RAG + OllamaLLM 연동 (정리)\n",
    "# ==========================\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# LLM 객체 생성\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\", temperature=0)\n",
    "# print(\"Step 8: OllamaLLM 객체 생성 완료\")\n",
    "\n",
    "# Prompt 정의\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical text analysis assistant.\n",
    "\n",
    "Extract drugs that were actually tested or administered in experiments from the following document chunks.\n",
    "Prefer drugs mentioned in 'Results' or 'Methods' sections and in Figures/Tables.\n",
    "Exclude drugs mentioned only in background, references, or literature.\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "- Merge WordPiece fragments into full drug names\n",
    "- Remove duplicates\n",
    "- List up to 3 most relevant drugs\n",
    "- Output as a semicolon-separated list\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\"])\n",
    "\n",
    "\n",
    "# RetrievalQA 체인 생성\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "# print(\"Step 8: RetrievalQA 체인 생성 완료\")\n",
    "\n",
    "# PDF 전체 text 합치기\n",
    "all_text = \" \".join([chunk.page_content for chunk in split_documents])\n",
    "# print(\"Step 8: 모든 chunk 합치기 완료, 길이:\", len(all_text))\n",
    "\n",
    "# RAG + LLM 실행\n",
    "response = chain({\"query\": all_text})\n",
    "rag_drugs = response['result']\n",
    "print(\"Step 8: RAG + LLM로 추출된 Drug Names:\\n\", rag_drugs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d6093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
