{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3401aa6",
   "metadata": {},
   "source": [
    "RAG-LangChain llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f3f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"/data1/workspace/pdfs/0008-5472_can-12-4183v1.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"  # 기본 Ollama 서버 주소\n",
    ")\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ca39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Any\n",
    "import subprocess\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"gpt-oss\"\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        result = subprocess.run(\n",
    "            ['ollama', 'run', self.model_name, prompt],\n",
    "            capture_output=True, \n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Ollama LLM failed: {result.stderr}\")\n",
    "        return result.stdout.strip()\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model_name\": self.model_name, \"temperature\": self.temperature}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ollama\"\n",
    "    \n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"  # 또는 \"map_reduce\", \"refine\", \"map_rerank\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bebbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, List, Any\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str = \"gpt-oss\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    timeout: int = 300  # 13GB 모델이라 넉넉히 설정\n",
    "    \n",
    "    def _call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None\n",
    "    ) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "        except requests.Timeout:\n",
    "            raise RuntimeError(f\"Ollama timed out after {self.timeout}s\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {str(e)}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "# 사용\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a81c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580745/2061240503.py:3: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  print(llm(\"What is 2+2?\"))\n",
      "/tmp/ipykernel_580745/2061240503.py:13: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain({\"query\": \"논문에서 주된 신약명은?\"})\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "논문에서 주된 신약명은 **스위티니부(Sunitinib)** 입니다.\n"
     ]
    }
   ],
   "source": [
    "# # 1. LLM 직접 테스트\n",
    "llm = OllamaLLM(model_name=\"gpt-oss\")\n",
    "print(llm(\"What is 2+2?\"))\n",
    "\n",
    "# 2. 체인 실행\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = chain({\"query\": \"논문에서 주된 신약명은?\"})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae605401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "sunitinib; pazopanib; DC101; paclitaxel; cyclophosphamide; tegafur; uracil; 5-fluorouracil\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 커스텀 프롬프트 정의\n",
    "prompt_template = \"\"\"\n",
    "You are a biomedical literature analysis assistant specialized in pharmacology and oncology.\n",
    "\n",
    "==== Document Excerpt Start ====\n",
    "{context}\n",
    "==== Document Excerpt End ====\n",
    "\n",
    "Task:\n",
    "Extract all **drug names** (both generic and brand names) that are experimentally **tested, administered, or evaluated** in the study.\n",
    "\n",
    "Follow these expert-level rules:\n",
    "\n",
    "1. Include drugs explicitly mentioned in the **Results** or **Methods** sections as being:\n",
    "   - administered, treated, exposed, injected, or tested,  \n",
    "   - or associated with measurable outcomes such as tumor volume, cancer size, toxicity, response rate, or doses tested.\n",
    "2. Include drugs that appear in **figures or tables**, even if not mentioned in the main text.\n",
    "   - e.g., drugs listed in figure captions, chart labels, or legends.\n",
    "3. Exclude drugs that appear **only** in the 'References' section.\n",
    "4. Include both **generic names** (e.g., cisplatin) and **brand names** (e.g., Platinol).\n",
    "5. Exclude gene names, protein names, molecular targets, and pathway components (e.g., EGFR, HER2, mTOR).\n",
    "6. Use logical reasoning to determine whether each drug was **actually used or tested** in the described experiments — not just mentioned conceptually or mechanistically.\n",
    "7. Extract only unique names (no duplicates).\n",
    "\n",
    "---\n",
    "\n",
    "Think step by step:\n",
    "1. Identify all candidate drug names (generic or brand).  \n",
    "2. Check whether the text or figures indicate that these drugs were **experimentally used, tested, or administered**.  \n",
    "3. Exclude any names mentioned solely in the References or in mechanistic/pathway discussions.  \n",
    "4. Combine both text-based and figure-based mentions into one list.  \n",
    "5. Output the final unique list of drug names that were experimentally involved in the study.\n",
    "\n",
    "Output format:\n",
    "List all unique drug names separated by semicolons (;), with no explanation or numbering.\n",
    "\n",
    "Example output:\n",
    "cisplatin; afatinib; Gilotrif; erlotinib\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# RetrievalQA에 적용\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 실행\n",
    "question = \"Extract generic drug names from this paper\"\n",
    "response = chain({\"query\": question})\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
