{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbe5a51",
   "metadata": {},
   "source": [
    "Í∞ÄÏÉÅÌôòÍ≤Ω : BASE(PYTHON 3.11.13) ÏúºÎ°ú ÏßÑÌñâ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 15:34:11.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.737 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.738 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.740 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.740 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.742 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.745 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.747 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.751 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-21 15:34:11.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# app_rag.py\n",
    "import streamlit as st\n",
    "import fitz                # pip install pymupdf\n",
    "import requests\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss               # pip install faiss-cpu (or faiss-gpu)\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# -----------------------\n",
    "# ÏÑ§Ï†ï\n",
    "# -----------------------\n",
    "OLLAMA_API = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"gpt-oss\"        # Î°úÏª¨ÏóêÏÑú ÎèåÏïÑÍ∞ÄÎäî Î™®Îç∏ Ïù¥Î¶Ñ (Ïã§ÌñâÌôòÍ≤ΩÏóê ÎßûÏ∂∞ Î≥ÄÍ≤Ω)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 800               # Ï≤≠ÌÅ¨ Í∏∏Ïù¥ (Î¨∏Ïûê)\n",
    "CHUNK_OVERLAP = 150\n",
    "TOP_K = 5                      # Í≤ÄÏÉâÌï† Ï≤≠ÌÅ¨ Í∞úÏàò\n",
    "EMBED_DIM = 384                # all-MiniLM-L6-v2 ÏûÑÎ≤†Îî© Ï∞®Ïõê (Î™®Îç∏ Í∏∞Ï§Ä)\n",
    "\n",
    "# -----------------------\n",
    "# Ïú†Ìã∏: ÌååÏùº Ìï¥Ïãú\n",
    "# -----------------------\n",
    "def file_hash_bytes(data: bytes) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(data)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# -----------------------\n",
    "# PDF -> ÌÖçÏä§Ìä∏\n",
    "# -----------------------\n",
    "def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:\n",
    "    pdf = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    pages = []\n",
    "    for p in pdf:\n",
    "        pages.append(p.get_text())\n",
    "    return \"\\n\\n\".join(pages).strip()\n",
    "\n",
    "# -----------------------\n",
    "# ÌÖçÏä§Ìä∏Î•º Ï≤≠ÌÅ¨Î°ú Î∂ÑÌï†\n",
    "# -----------------------\n",
    "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = min(start + chunk_size, L)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == L:\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# -----------------------\n",
    "# ÏûÑÎ≤†Îî© Ï§ÄÎπÑ (Ï∫êÏãú)\n",
    "# -----------------------\n",
    "@st.cache_resource\n",
    "def load_embedder():\n",
    "    return SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# -----------------------\n",
    "# FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ±\n",
    "# -----------------------\n",
    "def build_faiss_index(embeddings: np.ndarray):\n",
    "    # L2 ÎÇ¥Ï†Å Ïú†ÏÇ¨ÎèÑÏö© Ïù∏Îç±Ïä§ (Normalized cosine)\n",
    "    index = faiss.IndexFlatIP(EMBED_DIM)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# -----------------------\n",
    "# Ollama Ìò∏Ï∂ú (ÏïàÏ†Ñ Ï≤òÎ¶¨)\n",
    "# -----------------------\n",
    "def call_ollama(prompt: str, model: str = OLLAMA_MODEL, timeout=120):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.post(OLLAMA_API, json=payload, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        # Ollama ÏùëÎãµ Ìè¨Îß∑ ÏïàÏ†Ñ Ï≤òÎ¶¨\n",
    "        if isinstance(data, dict):\n",
    "            if \"response\" in data:\n",
    "                return data[\"response\"], None\n",
    "            else:\n",
    "                # Ï†ÑÏ≤¥ JSONÏùÑ ÏóêÎü¨Î°ú Î≥¥Ïó¨Ï£ºÍ∏∞\n",
    "                return None, data\n",
    "        else:\n",
    "            return None, {\"error\": \"unexpected response type\", \"raw\": data}\n",
    "    except Exception as e:\n",
    "        return None, {\"error\": str(e)}\n",
    "\n",
    "# -----------------------\n",
    "# RAG ÏßàÏùò Ï≤òÎ¶¨: ÏßàÎ¨∏ Î¶¨Ïä§Ìä∏ ÏàúÏ∞® Ïã§Ìñâ\n",
    "# -----------------------\n",
    "def rag_answer_questions(questions, retriever, chunks, embedder):\n",
    "    results = {}\n",
    "    for q in questions:\n",
    "        # ÏûÑÎ≤†Îî©: ÏßàÎ¨∏ ÏûÑÎ≤†Îî©\n",
    "        q_emb = embedder.encode([q], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        D, I = retriever.search(q_emb, TOP_K)  # I shape (1, k)\n",
    "        idxs = I[0].tolist()\n",
    "        context = \"\\n\\n---\\n\\n\".join([chunks[i] for i in idxs if i < len(chunks)])\n",
    "        # ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø: Î™ÖÌôïÌïú Ìè¨Îß∑ ÏöîÏ≤≠\n",
    "        prompt = (\n",
    "            f\"Îã§Ïùå Î¨∏ÏÑú Î∞úÏ∑åÎ¨∏ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏïÑÎûò ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.\\n\\n\"\n",
    "            f\"==== Î¨∏ÏÑú Î∞úÏ∑å ÏãúÏûë ====\\n{context}\\n==== Î¨∏ÏÑú Î∞úÏ∑å ÎÅù ====\\n\\n\"\n",
    "            f\"ÏßàÎ¨∏: {q}\\n\"\n",
    "            f\"ÏöîÍµ¨ÏÇ¨Ìï≠:\\n\"\n",
    "            f\"1) Í∞ÑÍ≤∞ÌïòÍ≤å ÌïµÏã¨Îßå ÏöîÏïΩ\\n\"\n",
    "            f\"2) Í≤∞Í≥ºÎ•º 'Ìï≠Î™©Î™Ö: Í∞í' ÌòïÌÉúÎ°ú Ï∂úÎ†•\\n\"\n",
    "            f\"3) Í∞ÄÎä•ÌïòÎ©¥ Í∑ºÍ±∞ Î¨∏Ïû•(Î¨∏ÏÑúÏóêÏÑú Î∞úÏ∑åÌïú Î¨∏Ïû•)ÏùÑ ÏßßÍ≤å Ìï®Íªò ÎçßÎ∂ôÏùº Í≤É\\n\"\n",
    "            f\"ÏùëÎãµ:\"\n",
    "        )\n",
    "        answer, err = call_ollama(prompt)\n",
    "        if err:\n",
    "            results[q] = {\"answer\": None, \"error\": err, \"context_idxs\": idxs}\n",
    "        else:\n",
    "            results[q] = {\"answer\": answer, \"error\": None, \"context_idxs\": idxs}\n",
    "    return results\n",
    "\n",
    "# -----------------------\n",
    "# Streamlit UI\n",
    "# -----------------------\n",
    "st.set_page_config(page_title=\"PDF ‚Üí RAG ‚Üí GPT-OSS (Ïã†ÏïΩ Ï†ïÎ≥¥ ÏûêÎèôÌôî)\", layout=\"wide\")\n",
    "st.title(\"üî¨ PDF ‚Üí RAG ‚Üí GPT-OSS : Ïã†ÏïΩÎ™Ö / Ìö®Îä• / ÎèÖÏÑ± ÏûêÎèô Ï∂îÏ∂ú\")\n",
    "\n",
    "st.markdown(\"ÏóÖÎ°úÎìú Îêú ÎÖºÎ¨∏(PDF)ÏóêÏÑú **Ïã†ÏïΩÎ™Ö / Ìö®Îä•(responsive) / ÎèÖÏÑ±(toxic)** Ìï≠Î™©ÏùÑ ÏûêÎèôÏúºÎ°ú Ï∂îÏ∂úÌï©ÎãàÎã§. Ollama(gpt-oss)ÏôÄ Î°úÏª¨ ÏûÑÎ≤†Îî©+FAISSÎ•º ÏÇ¨Ïö©Ìïú RAG Î∞©ÏãùÏûÖÎãàÎã§.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"ÎÖºÎ¨∏(PDF) ÏóÖÎ°úÎìú\", type=[\"pdf\"])\n",
    "run_button = st.button(\"Î∂ÑÏÑù ÏãúÏûë\")\n",
    "\n",
    "if uploaded_file and run_button:\n",
    "    raw = uploaded_file.read()\n",
    "    fid = file_hash_bytes(raw)\n",
    "    st.info(f\"ÌååÏùº Ìï¥Ïãú: {fid[:12]}... (Ï∫êÏã±Ïö©)\")\n",
    "    with st.spinner(\"PDFÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ï§ë...\"):\n",
    "        text = extract_text_from_pdf_bytes(raw)\n",
    "\n",
    "    if not text:\n",
    "        st.error(\"PDFÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§. Ïä§Ï∫îÌïú Ïù¥ÎØ∏ÏßÄÌòï PDFÏù∏ Í≤ΩÏö∞ OCR ÌïÑÏöî.\")\n",
    "    else:\n",
    "        st.success(f\"Ï¥ù Î¨∏Ïûê Ïàò: {len(text)}\")\n",
    "        # Ï≤≠ÌÅ¨ Î∂ÑÌï†\n",
    "        chunks = chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "        st.write(f\"ÏÉùÏÑ±Îêú Ï≤≠ÌÅ¨ Ïàò: {len(chunks)} (Ï≤≠ÌÅ¨ Í∏∏Ïù¥={CHUNK_SIZE}, Í≤πÏπ®={CHUNK_OVERLAP})\")\n",
    "\n",
    "        # ÏûÑÎ≤†Îî© & FAISS (Ï∫êÏãú ÌååÏùº Ìè¥Îçî ÏÇ¨Ïö©)\n",
    "        cache_dir = os.path.join(\".cache_rag\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        index_path = os.path.join(cache_dir, f\"{fid}.index\")\n",
    "        chunks_path = os.path.join(cache_dir, f\"{fid}_chunks.json\")\n",
    "\n",
    "        embedder = load_embedder()\n",
    "\n",
    "        if os.path.exists(index_path) and os.path.exists(chunks_path):\n",
    "            try:\n",
    "                st.info(\"Ïù¥ ÌååÏùºÏóê ÎåÄÌïú FAISS Ïù∏Îç±Ïä§ Ï∫êÏãúÎ•º Î°úÎìúÌï©ÎãàÎã§.\")\n",
    "                idx = faiss.read_index(index_path)\n",
    "                with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    chunks = json.load(f)\n",
    "                retriever = idx\n",
    "            except Exception as e:\n",
    "                st.warning(\"Ï∫êÏãú Î°úÎìú Ïã§Ìå®, Ïù∏Îç±Ïä§ Ïû¨ÏÉùÏÑ±Ìï©ÎãàÎã§.\")\n",
    "                os.remove(index_path) if os.path.exists(index_path) else None\n",
    "                # fallthrough to create\n",
    "                retriever = None\n",
    "        else:\n",
    "            retriever = None\n",
    "\n",
    "        if retriever is None:\n",
    "            with st.spinner(\"ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è FAISS Ïù∏Îç±Ïä§ Íµ¨Ï∂ï Ï§ë... (ÏïΩÍ∞Ñ ÏãúÍ∞Ñ ÏÜåÏöî)\"):\n",
    "                # Î∞∞Ïπò ÏûÑÎ≤†Îî©\n",
    "                batch_size = 64\n",
    "                embeddings = []\n",
    "                for i in tqdm(range(0, len(chunks), batch_size), desc=\"Embedding\"):\n",
    "                    batch = chunks[i:i+batch_size]\n",
    "                    embs = embedder.encode(batch, convert_to_numpy=True)\n",
    "                    embeddings.append(embs)\n",
    "                embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "                # Build index\n",
    "                if embeddings.shape[1] != EMBED_DIM:\n",
    "                    st.warning(f\"ÏûÑÎ≤†Îî© Ï∞®Ïõê {embeddings.shape[1]} (ÏòàÏÉÅ {EMBED_DIM})ÏûÖÎãàÎã§. EMBED_DIM Í∞íÏùÑ ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.\")\n",
    "                faiss.normalize_L2(embeddings)\n",
    "                retriever = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "                retriever.add(embeddings)\n",
    "                # save index and chunks\n",
    "                faiss.write_index(retriever, index_path)\n",
    "                with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(chunks, f, ensure_ascii=False)\n",
    "                st.success(\"FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ± ÏôÑÎ£å Î∞è Ï∫êÏãú Ï†ÄÏû•Îê®.\")\n",
    "\n",
    "        # ÏûêÎèôÏßàÎ¨∏ Î¶¨Ïä§Ìä∏\n",
    "        questions = [\n",
    "            \"ÎÖºÎ¨∏ÏóêÏÑú Ïñ∏Í∏âÎêú Ïã†ÏïΩÎ™Ö(ÌõÑÎ≥¥)ÏùÑ ÏïåÎ†§Ï§ò.\",\n",
    "            \"Ïù¥ Ïã†ÏïΩÏùò Ìö®Îä•(responsive)Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï§ò (Ïñ¥Îñ§ Ï†ÅÏùëÏ¶ù/Î∞òÏùëÏùÑ Î≥¥ÏòÄÎäîÏßÄ).\",\n",
    "            \"Ïù¥ Ïã†ÏïΩÏùò ÎèÖÏÑ±(toxic) Í¥ÄÎ†® Ïñ∏Í∏âÏù¥ ÏûàÎäîÏßÄ, ÏûàÎã§Î©¥ Ïñ¥Îñ§ ÎèÖÏÑ±Ïù∏ÏßÄ ÎßêÌï¥Ï§ò.\"\n",
    "        ]\n",
    "\n",
    "        st.info(\"Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ï≤≠ÌÅ¨Î•º Í∏∞Î∞òÏúºÎ°ú Ollama(gpt-oss)Ïóê ÏßàÏùòÌï©ÎãàÎã§...\")\n",
    "        results = rag_answer_questions(questions, retriever, chunks, embedder)\n",
    "\n",
    "        # Í≤∞Í≥º ÌëúÏãú\n",
    "        for q in questions:\n",
    "            st.markdown(\"---\")\n",
    "            st.subheader(q)\n",
    "            entry = results[q]\n",
    "            if entry[\"error\"]:\n",
    "                st.error(f\"ÏßàÏùò Ïã§Ìå®: {entry['error']}\")\n",
    "                st.write(\"Í¥ÄÎ†® Ï≤≠ÌÅ¨ Ïù∏Îç±Ïä§:\", entry.get(\"context_idxs\"))\n",
    "            else:\n",
    "                st.write(entry[\"answer\"])\n",
    "                # Í∞ÑÎûµÌïòÍ≤å Í¥ÄÎ†® Ï≤≠ÌÅ¨ÎèÑ Î≥¥Ïó¨Ï£ºÍ∏∞\n",
    "                st.markdown(\"**Ï∞∏Ï°∞Îêú Î¨∏Ïû•(Ï≤≠ÌÅ¨):**\")\n",
    "                for idx in entry.get(\"context_idxs\", []):\n",
    "                    if idx < len(chunks):\n",
    "                        st.write(f\"- (idx {idx}) \" + chunks[idx][:400].replace(\"\\n\", \" \") + (\"...\" if len(chunks[idx]) > 400 else \"\"))\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"Tip: Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞, overlap, top_k Í∞íÏùÄ Î¨∏ÏÑú ÌäπÏÑ±Ïóê Îî∞Îùº Ï°∞Ï†àÌïòÏÑ∏Ïöî.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043468f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0eeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
