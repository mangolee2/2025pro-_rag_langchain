{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c296134",
   "metadata": {},
   "source": [
    "1. app_ ì‹¤í–‰ì½”ë“œëŠ” ê°€ìƒí™˜ê²½ baseì—ì„œ ì‹¤í–‰í•  ê²ƒ\n",
    "1-2. í„°ë¯¸ë„ì—ì„œë„ í™œì„±í™” ì‹œí‚¬ ê²ƒ\n",
    "2. streamlit run ~ ìœ¼ë¡œ í„°ë¯¸ë„(bash)ì—ì„œ ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce4bd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e06f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_rag.py\n",
    "import streamlit as st\n",
    "import fitz                # pip install pymupdf\n",
    "import requests\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss               # pip install faiss-cpu (or faiss-gpu)\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# -----------------------\n",
    "# ì„¤ì •\n",
    "# -----------------------\n",
    "OLLAMA_API = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"gpt-oss\"        # ë¡œì»¬ì—ì„œ ëŒì•„ê°€ëŠ” ëª¨ë¸ ì´ë¦„ (ì‹¤í–‰í™˜ê²½ì— ë§ì¶° ë³€ê²½)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 800               # ì²­í¬ ê¸¸ì´ (ë¬¸ì)\n",
    "CHUNK_OVERLAP = 150\n",
    "TOP_K = 5                      # ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜\n",
    "EMBED_DIM = 384                # all-MiniLM-L6-v2 ì„ë² ë”© ì°¨ì› (ëª¨ë¸ ê¸°ì¤€)\n",
    "\n",
    "# -----------------------\n",
    "# ìœ í‹¸: íŒŒì¼ í•´ì‹œ\n",
    "# -----------------------\n",
    "def file_hash_bytes(data: bytes) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(data)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# -----------------------\n",
    "# PDF -> í…ìŠ¤íŠ¸\n",
    "# -----------------------\n",
    "def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:\n",
    "    pdf = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    pages = []\n",
    "    for p in pdf:\n",
    "        pages.append(p.get_text())\n",
    "    return \"\\n\\n\".join(pages).strip()\n",
    "\n",
    "# -----------------------\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "# -----------------------\n",
    "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = min(start + chunk_size, L)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == L:\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# -----------------------\n",
    "# ì„ë² ë”© ì¤€ë¹„ (ìºì‹œ)\n",
    "# -----------------------\n",
    "@st.cache_resource\n",
    "def load_embedder():\n",
    "    return SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# -----------------------\n",
    "# FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "# -----------------------\n",
    "def build_faiss_index(embeddings: np.ndarray):\n",
    "    # L2 ë‚´ì  ìœ ì‚¬ë„ìš© ì¸ë±ìŠ¤ (Normalized cosine)\n",
    "    index = faiss.IndexFlatIP(EMBED_DIM)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# -----------------------\n",
    "# Ollama í˜¸ì¶œ (ì•ˆì „ ì²˜ë¦¬)\n",
    "# -----------------------\n",
    "def call_ollama(prompt: str, model: str = OLLAMA_MODEL, timeout=120):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.post(OLLAMA_API, json=payload, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        # Ollama ì‘ë‹µ í¬ë§· ì•ˆì „ ì²˜ë¦¬\n",
    "        if isinstance(data, dict):\n",
    "            if \"response\" in data:\n",
    "                return data[\"response\"], None\n",
    "            else:\n",
    "                # ì „ì²´ JSONì„ ì—ëŸ¬ë¡œ ë³´ì—¬ì£¼ê¸°\n",
    "                return None, data\n",
    "        else:\n",
    "            return None, {\"error\": \"unexpected response type\", \"raw\": data}\n",
    "    except Exception as e:\n",
    "        return None, {\"error\": str(e)}\n",
    "\n",
    "# -----------------------\n",
    "# RAG ì§ˆì˜ ì²˜ë¦¬: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ìˆœì°¨ ì‹¤í–‰\n",
    "# -----------------------\n",
    "def rag_answer_questions(questions, retriever, chunks, embedder):\n",
    "    results = {}\n",
    "    for q in questions:\n",
    "        # ì„ë² ë”©: ì§ˆë¬¸ ì„ë² ë”©\n",
    "        q_emb = embedder.encode([q], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        D, I = retriever.search(q_emb, TOP_K)  # I shape (1, k)\n",
    "        idxs = I[0].tolist()\n",
    "        context = \"\\n\\n---\\n\\n\".join([chunks[i] for i in idxs if i < len(chunks)])\n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ëª…í™•í•œ í¬ë§· ìš”ì²­\n",
    "        prompt = (\n",
    "            f\"ë‹¤ìŒ ë¬¸ì„œ ë°œì·Œë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\n\"\n",
    "            f\"==== ë¬¸ì„œ ë°œì·Œ ì‹œì‘ ====\\n{context}\\n==== ë¬¸ì„œ ë°œì·Œ ë ====\\n\\n\"\n",
    "            f\"ì§ˆë¬¸: {q}\\n\"\n",
    "            f\"ìš”êµ¬ì‚¬í•­:\\n\"\n",
    "            # f\"1) ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ìš”ì•½\\n\"\n",
    "            # f\"2) ê²°ê³¼ë¥¼ 'í•­ëª©ëª…: ê°’' í˜•íƒœë¡œ ì¶œë ¥\\n\"\n",
    "            f\"1) ê°€ëŠ¥í•˜ë©´ ê·¼ê±° ë¬¸ì¥(ë¬¸ì„œì—ì„œ ë°œì·Œí•œ ë¬¸ì¥)ì„ ì§§ê²Œ í•¨ê»˜ ë§ë¶™ì¼ ê²ƒ\\n\"\n",
    "            f\"2) 1ë²ˆê³¼ ë³„ê°œë¡œ, ìš”ì•½í•´ì„œ í…Œì´ë¸” í˜•íƒœë¡œ ì •ë¦¬ ì¶œë ¥\"\n",
    "            f\"ì‘ë‹µ:\"\n",
    "        )\n",
    "        answer, err = call_ollama(prompt)\n",
    "        if err:\n",
    "            results[q] = {\"answer\": None, \"error\": err, \"context_idxs\": idxs}\n",
    "        else:\n",
    "            results[q] = {\"answer\": answer, \"error\": None, \"context_idxs\": idxs}\n",
    "    return results\n",
    "\n",
    "# -----------------------\n",
    "# Streamlit UI\n",
    "# -----------------------\n",
    "st.set_page_config(page_title=\"PDF â†’ RAG â†’ GPT-OSS (ì‹ ì•½ ì •ë³´ ìë™í™”)\", layout=\"wide\")\n",
    "st.title(\"ğŸ”¬ PDF â†’ RAG â†’ GPT-OSS : ì‹ ì•½ëª… / íš¨ëŠ¥ / ë…ì„± ìë™ ì¶”ì¶œ\")\n",
    "\n",
    "st.markdown(\"ì—…ë¡œë“œ ëœ ë…¼ë¬¸(PDF)ì—ì„œ **ì‹ ì•½ëª… / íš¨ëŠ¥(responsive) / ë…ì„±(toxic)** í•­ëª©ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. Ollama(gpt-oss)ì™€ ë¡œì»¬ ì„ë² ë”©+FAISSë¥¼ ì‚¬ìš©í•œ RAG ë°©ì‹ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"ë…¼ë¬¸(PDF) ì—…ë¡œë“œ\", type=[\"pdf\"])\n",
    "run_button = st.button(\"ë¶„ì„ ì‹œì‘\")\n",
    "\n",
    "if uploaded_file and run_button:\n",
    "    raw = uploaded_file.read()\n",
    "    fid = file_hash_bytes(raw)\n",
    "    st.info(f\"íŒŒì¼ í•´ì‹œ: {fid[:12]}... (ìºì‹±ìš©)\")\n",
    "    with st.spinner(\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\"):\n",
    "        text = extract_text_from_pdf_bytes(raw)\n",
    "\n",
    "    if not text:\n",
    "        st.error(\"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”í•œ ì´ë¯¸ì§€í˜• PDFì¸ ê²½ìš° OCR í•„ìš”.\")\n",
    "    else:\n",
    "        st.success(f\"ì´ ë¬¸ì ìˆ˜: {len(text)}\")\n",
    "        # ì²­í¬ ë¶„í• \n",
    "        chunks = chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "        st.write(f\"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)} (ì²­í¬ ê¸¸ì´={CHUNK_SIZE}, ê²¹ì¹¨={CHUNK_OVERLAP})\")\n",
    "\n",
    "        # ì„ë² ë”© & FAISS (ìºì‹œ íŒŒì¼ í´ë” ì‚¬ìš©)\n",
    "        cache_dir = os.path.join(\".cache_rag\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        index_path = os.path.join(cache_dir, f\"{fid}.index\")\n",
    "        chunks_path = os.path.join(cache_dir, f\"{fid}_chunks.json\")\n",
    "\n",
    "        embedder = load_embedder()\n",
    "\n",
    "        if os.path.exists(index_path) and os.path.exists(chunks_path):\n",
    "            try:\n",
    "                st.info(\"ì´ íŒŒì¼ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ìºì‹œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "                idx = faiss.read_index(index_path)\n",
    "                with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    chunks = json.load(f)\n",
    "                retriever = idx\n",
    "            except Exception as e:\n",
    "                st.warning(\"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¸ë±ìŠ¤ ì¬ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "                os.remove(index_path) if os.path.exists(index_path) else None\n",
    "                # fallthrough to create\n",
    "                retriever = None\n",
    "        else:\n",
    "            retriever = None\n",
    "\n",
    "        if retriever is None:\n",
    "            with st.spinner(\"ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘... (ì•½ê°„ ì‹œê°„ ì†Œìš”)\"):\n",
    "                # ë°°ì¹˜ ì„ë² ë”©\n",
    "                batch_size = 64\n",
    "                embeddings = []\n",
    "                for i in tqdm(range(0, len(chunks), batch_size), desc=\"Embedding\"):\n",
    "                    batch = chunks[i:i+batch_size]\n",
    "                    embs = embedder.encode(batch, convert_to_numpy=True)\n",
    "                    embeddings.append(embs)\n",
    "                embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "                # Build index\n",
    "                if embeddings.shape[1] != EMBED_DIM:\n",
    "                    st.warning(f\"ì„ë² ë”© ì°¨ì› {embeddings.shape[1]} (ì˜ˆìƒ {EMBED_DIM})ì…ë‹ˆë‹¤. EMBED_DIM ê°’ì„ ë§ì¶°ì£¼ì„¸ìš”.\")\n",
    "                faiss.normalize_L2(embeddings)\n",
    "                retriever = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "                retriever.add(embeddings)\n",
    "                # save index and chunks\n",
    "                faiss.write_index(retriever, index_path)\n",
    "                with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(chunks, f, ensure_ascii=False)\n",
    "                st.success(\"FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ë° ìºì‹œ ì €ì¥ë¨.\")\n",
    "\n",
    "        # ìë™ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "        questions = [\n",
    "            \"ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì‹ ì•½ëª…ë“¤(í›„ë³´)ì„ ì•Œë ¤ì¤˜.\",\n",
    "            \"ê° ì‹ ì•½ì˜ ìœ íš¨ì„±(responsive)ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜ (ìœ íš¨ì„±ì´ë€, ì•”ì˜ ë¬´ê²Œì™€ ë¶€í”¼ì˜ ê°ì†Œ ë° ë³€í™” ìˆ˜ì¹˜, í˜¹ì€ í¼ì„¼í…Œì´ì§€).\",\n",
    "            \"ê° ì‹ ì•½ì˜ ë…ì„±(toxic) ê´€ë ¨ ì •ë³´ë¥¼ ë§í•´ì¤˜.(ë…ì„±ì€, ì „ì²´ ì¥ ì¤‘ ëª‡ ë§ˆë¦¬ê°€ ì‚¬ë§í–ˆëŠ”ì§€ í¼ì„¼íŠ¸ í˜¹ì€ o,x í˜•íƒœë¡œ)\"\n",
    "        ]\n",
    "\n",
    "        st.info(\"ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Ollama(gpt-oss)ì— ì§ˆì˜í•©ë‹ˆë‹¤...\")\n",
    "        results = rag_answer_questions(questions, retriever, chunks, embedder)\n",
    "\n",
    "        # ê²°ê³¼ í‘œì‹œ\n",
    "        for q in questions:\n",
    "            st.markdown(\"---\")\n",
    "            st.subheader(q)\n",
    "            entry = results[q]\n",
    "            if entry[\"error\"]:\n",
    "                st.error(f\"ì§ˆì˜ ì‹¤íŒ¨: {entry['error']}\")\n",
    "                st.write(\"ê´€ë ¨ ì²­í¬ ì¸ë±ìŠ¤:\", entry.get(\"context_idxs\"))\n",
    "            else:\n",
    "                st.write(entry[\"answer\"])\n",
    "                # ê°„ëµí•˜ê²Œ ê´€ë ¨ ì²­í¬ë„ ë³´ì—¬ì£¼ê¸°\n",
    "                st.markdown(\"**ì°¸ì¡°ëœ ë¬¸ì¥(ì²­í¬):**\")\n",
    "                for idx in entry.get(\"context_idxs\", []):\n",
    "                    if idx < len(chunks):\n",
    "                        st.write(f\"- (idx {idx}) \" + chunks[idx][:400].replace(\"\\n\", \" \") + (\"...\" if len(chunks[idx]) > 400 else \"\"))\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"Tip: ì²­í¬ í¬ê¸°, overlap, top_k ê°’ì€ ë¬¸ì„œ íŠ¹ì„±ì— ë”°ë¼ ì¡°ì ˆí•˜ì„¸ìš”.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
