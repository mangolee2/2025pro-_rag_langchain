# app_rag.py
import streamlit as st
import fitz                # pip install pymupdf
import requests
import hashlib
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss               # pip install faiss-cpu (or faiss-gpu)
from tqdm import tqdm
import os
import json
from langchain.schema import SystemMessage, HumanMessage

# -----------------------
# ì„¤ì •
# -----------------------
OLLAMA_API = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.2-vision"        # ë¡œì»¬ì—ì„œ ëŒì•„ê°€ëŠ” ëª¨ë¸: ë©€í‹°ëª¨ë‹¬ ì”€ 
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 800               # ì²­í¬ ê¸¸ì´ (ë¬¸ì)
CHUNK_OVERLAP = 150
TOP_K = 5                      # ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜
EMBED_DIM = 384                # all-MiniLM-L6-v2 ì„ë² ë”© ì°¨ì› (ëª¨ë¸ ê¸°ì¤€)

# -----------------------
# ìœ í‹¸: íŒŒì¼ í•´ì‹œ
# -----------------------
def file_hash_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

# -----------------------
# PDF -> í…ìŠ¤íŠ¸
# -----------------------
def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    pdf = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    for p in pdf:
        pages.append(p.get_text())
    return "\n\n".join(pages).strip()

# -----------------------
# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
# -----------------------
def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    if not text:
        return []
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(start + chunk_size, L)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == L:
            break
        start = end - overlap
    return chunks

# -----------------------
# ì„ë² ë”© ì¤€ë¹„ (ìºì‹œ)
# -----------------------
@st.cache_resource
def load_embedder():
    return SentenceTransformer(EMBED_MODEL_NAME)

# -----------------------
# FAISS ì¸ë±ìŠ¤ ìƒì„±
# -----------------------
def build_faiss_index(embeddings: np.ndarray):
    # L2 ë‚´ì  ìœ ì‚¬ë„ìš© ì¸ë±ìŠ¤ (Normalized cosine)
    index = faiss.IndexFlatIP(EMBED_DIM)
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index

# -----------------------
# Ollama í˜¸ì¶œ (ì•ˆì „ ì²˜ë¦¬)
# -----------------------
def call_ollama(prompt: str, model: str = OLLAMA_MODEL, timeout=120):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    try:
        resp = requests.post(OLLAMA_API, json=payload, timeout=timeout)
        resp.raise_for_status()
        data = resp.json()
        # Ollama ì‘ë‹µ í¬ë§· ì•ˆì „ ì²˜ë¦¬
        if isinstance(data, dict):
            if "response" in data:
                return data["response"], None
            else:
                # ì „ì²´ JSONì„ ì—ëŸ¬ë¡œ ë³´ì—¬ì£¼ê¸°
                return None, data
        else:
            return None, {"error": "unexpected response type", "raw": data}
    except Exception as e:
        return None, {"error": str(e)}

# -----------------------
# RAG ì§ˆì˜ ì²˜ë¦¬: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ìˆœì°¨ ì‹¤í–‰
# -----------------------

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜
system_message = SystemMessage(content="you are a reasearcher for new drug development related to animal experiments. You have to explain generic names in papers.")


def rag_answer_questions(questions, retriever, chunks, embedder):
    results = {}
    for q in questions:
        # ì„ë² ë”©: ì§ˆë¬¸ ì„ë² ë”©
        q_emb = embedder.encode([q], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = retriever.search(q_emb, TOP_K)  # I shape (1, k)
        idxs = I[0].tolist()
        context = "\n\n---\n\n".join([chunks[i] for i in idxs if i < len(chunks)])
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ëª…í™•í•œ í¬ë§· ìš”ì²­
        prompt = (
                f"You are a biomedical text analysis assistant.\n\n"
                f"==== Document Excerpt Start ====\n{context}\n==== Document Excerpt End ====\n\n"
                f"Task:\n"
                f"- Extract only **generic (non-brand)** drug names mentioned in the 'Results' section of the provided research paper.\n"
                f"- If no drug names are found in the Results section, then extract them from the 'Methods' section instead.\n"
                f"- Exclude any drug names appearing in the 'References' section.\n"
                f"- Exclude gene names, protein names, biomarkers, molecular targets, and signaling pathway components "
                f"(e.g., EGFR, MEK, CDK4, PTEN, ERBB2, FGFR3, etc.).\n"
                f"- Exclude experimental or investigational code names (e.g., TAS0728, T-DM1, AZD9496, MK-2206, LY294002).\n"
                f"- Include only drugs that have an established or officially recognized generic name (INN).\n"
                f"- Extract only therapeutic agents or compounds actually tested, administered, or mentioned as drugs.\n"
                f"- List only essential and unique generic drug names.\n\n"
                f"Output format:\n"
                f"List all unique generic drug names separated by semicolons (;), without quotation marks.\n"
                f"Do not include any additional text, explanation, or numbering.\n\n"
                f"Example output:\n"
                f"nivolumab; pembrolizumab; sunitinib; fulvestrant\n\n"
                f"Question: {q}\n\n"
                f"Answer:"
            )

        answer, err = call_ollama(prompt)
        if err:
            results[q] = {"answer": None, "error": err, "context_idxs": idxs}
        else:
            results[q] = {"answer": answer, "error": None, "context_idxs": idxs}
    return results

# -----------------------
# Streamlit UI
# -----------------------
st.set_page_config(page_title="PDF â†’ RAG â†’ LLM (ì‹ ì•½ ì •ë³´ ìë™í™”)", layout="wide")
st.title("ğŸ”¬ PDF â†’ RAG â†’ LLM : ì‹ ì•½ëª… ìë™ ì¶”ì¶œ")

st.markdown("ì—…ë¡œë“œ ëœ ë…¼ë¬¸(PDF)ì—ì„œ **ì‹ ì•½ëª…** í•­ëª©ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. Ollamaì™€ ë¡œì»¬ ì„ë² ë”©+FAISSë¥¼ ì‚¬ìš©í•œ RAG ë°©ì‹ì…ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("ë…¼ë¬¸(PDF) ì—…ë¡œë“œ", type=["pdf"])
run_button = st.button("ë¶„ì„ ì‹œì‘")

if uploaded_file and run_button:
    raw = uploaded_file.read()
    fid = file_hash_bytes(raw)
    st.info(f"íŒŒì¼ í•´ì‹œ: {fid[:12]}... (ìºì‹±ìš©)")
    with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
        text = extract_text_from_pdf_bytes(raw)

    if not text:
        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”í•œ ì´ë¯¸ì§€í˜• PDFì¸ ê²½ìš° OCR í•„ìš”.")
    else:
        st.success(f"ì´ ë¬¸ì ìˆ˜: {len(text)}")
        # ì²­í¬ ë¶„í• 
        chunks = chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)
        st.write(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)} (ì²­í¬ ê¸¸ì´={CHUNK_SIZE}, ê²¹ì¹¨={CHUNK_OVERLAP})")

        # ì„ë² ë”© & FAISS (ìºì‹œ íŒŒì¼ í´ë” ì‚¬ìš©)
        cache_dir = os.path.join(".cache_rag")
        os.makedirs(cache_dir, exist_ok=True)
        index_path = os.path.join(cache_dir, f"{fid}.index")
        chunks_path = os.path.join(cache_dir, f"{fid}_chunks.json")

        embedder = load_embedder()

        if os.path.exists(index_path) and os.path.exists(chunks_path):
            try:
                st.info("ì´ íŒŒì¼ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ìºì‹œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
                idx = faiss.read_index(index_path)
                with open(chunks_path, "r", encoding="utf-8") as f:
                    chunks = json.load(f)
                retriever = idx
            except Exception as e:
                st.warning("ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¸ë±ìŠ¤ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                os.remove(index_path) if os.path.exists(index_path) else None
                # fallthrough to create
                retriever = None
        else:
            retriever = None

        if retriever is None:
            with st.spinner("ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘... (ì•½ê°„ ì‹œê°„ ì†Œìš”)"):
                # ë°°ì¹˜ ì„ë² ë”©
                batch_size = 64
                embeddings = []
                for i in tqdm(range(0, len(chunks), batch_size), desc="Embedding"):
                    batch = chunks[i:i+batch_size]
                    embs = embedder.encode(batch, convert_to_numpy=True)
                    embeddings.append(embs)
                embeddings = np.vstack(embeddings).astype("float32")
                # Build index
                if embeddings.shape[1] != EMBED_DIM:
                    st.warning(f"ì„ë² ë”© ì°¨ì› {embeddings.shape[1]} (ì˜ˆìƒ {EMBED_DIM})ì…ë‹ˆë‹¤. EMBED_DIM ê°’ì„ ë§ì¶°ì£¼ì„¸ìš”.")
                faiss.normalize_L2(embeddings)
                retriever = faiss.IndexFlatIP(embeddings.shape[1])
                retriever.add(embeddings)
                # save index and chunks
                faiss.write_index(retriever, index_path)
                with open(chunks_path, "w", encoding="utf-8") as f:
                    json.dump(chunks, f, ensure_ascii=False)
                st.success("FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ë° ìºì‹œ ì €ì¥ë¨.")

        # ìë™ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        questions = [
            "ì‹ ì•½ëª… ì¶”ì¶œ"
        ]

        st.info("ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Ollama(gpt-oss)ì— ì§ˆì˜í•©ë‹ˆë‹¤...")
        results = rag_answer_questions(questions, retriever, chunks, embedder)

        # ê²°ê³¼ í‘œì‹œ
        for q in questions:
            st.markdown("---")
            st.subheader(q)
            entry = results[q]
            if entry["error"]:
                st.error(f"ì§ˆì˜ ì‹¤íŒ¨: {entry['error']}")
                st.write("ê´€ë ¨ ì²­í¬ ì¸ë±ìŠ¤:", entry.get("context_idxs"))
            else:
                st.write(entry["answer"])
                # ê°„ëµí•˜ê²Œ ê´€ë ¨ ì²­í¬ë„ ë³´ì—¬ì£¼ê¸°
                # st.markdown("**ì°¸ì¡°ëœ ë¬¸ì¥(ì²­í¬):**")
                # for idx in entry.get("context_idxs", []):
                    # if idx < len(chunks):
                        # st.write(f"- (idx {idx}) " + chunks[idx][:400].replace("\n", " ") + ("..." if len(chunks[idx]) > 400 else ""))
st.markdown("---")
st.caption("Tip: ì²­í¬ í¬ê¸°, overlap, top_k ê°’ì€ ë¬¸ì„œ íŠ¹ì„±ì— ë”°ë¼ ì¡°ì ˆí•˜ì„¸ìš”.")
