# app_rag.py
import streamlit as st
import fitz                # pip install pymupdf
import requests
import hashlib
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss               # pip install faiss-cpu (or faiss-gpu)
from tqdm import tqdm
import os
import json
from langchain.schema import SystemMessage, HumanMessage
from langchain_core.prompts.few_shot import FewShotPromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# -----------------------
# ì„¤ì •
# -----------------------
OLLAMA_API = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "gpt-oss"        # ë¡œì»¬ì—ì„œ ëŒì•„ê°€ëŠ” ëª¨ë¸ ì´ë¦„ (ì‹¤í–‰í™˜ê²½ì— ë§ì¶° ë³€ê²½)
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 800               # ì²­í¬ ê¸¸ì´ (ë¬¸ì)
CHUNK_OVERLAP = 150
TOP_K = 5                      # ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜
EMBED_DIM = 384                # all-MiniLM-L6-v2 ì„ë² ë”© ì°¨ì› (ëª¨ë¸ ê¸°ì¤€)

# -----------------------
# ìœ í‹¸: íŒŒì¼ í•´ì‹œ
# -----------------------
def file_hash_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

# -----------------------
# PDF -> í…ìŠ¤íŠ¸
# -----------------------
def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    pdf = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    for p in pdf:
        pages.append(p.get_text())
    return "\n\n".join(pages).strip()

# -----------------------
# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
# -----------------------
def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    if not text:
        return []
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(start + chunk_size, L)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == L:
            break
        start = end - overlap
    return chunks

# -----------------------
# ì„ë² ë”© ì¤€ë¹„ (ìºì‹œ)
# -----------------------
@st.cache_resource
def load_embedder():
    return SentenceTransformer(EMBED_MODEL_NAME)

# -----------------------
# FAISS ì¸ë±ìŠ¤ ìƒì„±
# -----------------------
def build_faiss_index(embeddings: np.ndarray):
    # L2 ë‚´ì  ìœ ì‚¬ë„ìš© ì¸ë±ìŠ¤ (Normalized cosine)
    index = faiss.IndexFlatIP(EMBED_DIM)
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index

# -----------------------
# Ollama í˜¸ì¶œ (ì•ˆì „ ì²˜ë¦¬)
# -----------------------
def call_ollama(prompt: str, model: str = OLLAMA_MODEL, timeout=120):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    try:
        resp = requests.post(OLLAMA_API, json=payload, timeout=timeout)
        resp.raise_for_status()
        data = resp.json()
        # Ollama ì‘ë‹µ í¬ë§· ì•ˆì „ ì²˜ë¦¬
        if isinstance(data, dict):
            if "response" in data:
                return data["response"], None
            else:
                # ì „ì²´ JSONì„ ì—ëŸ¬ë¡œ ë³´ì—¬ì£¼ê¸°
                return None, data
        else:
            return None, {"error": "unexpected response type", "raw": data}
    except Exception as e:
        return None, {"error": str(e)}

# -----------------------
# RAG ì§ˆì˜ ì²˜ë¦¬: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ìˆœì°¨ ì‹¤í–‰
# -----------------------

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€: gpt ê³„ì—´ ëª¨ë¸ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì‹œ
# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜
system_message = SystemMessage(content="ë„ˆëŠ” mouseê´€ë ¨ ì‹ ì•½ ì—°êµ¬ì›ì´ì•¼. ì‹ ì•½ëª…ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆê³ , ë…¼ë¬¸ì—ì„œ ì´ë¥¼ ì˜ ì¶”ì¶œí•˜ëŠ” ê²Œ ë„¤ ì—­í• ì´ì•¼.  ")

examples = [
    {
        "question": "ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì‹ ì•½ëª…(í›„ë³´ë“¤)ì´ ë­ì•¼?",
        "answer": """Trametinib;Palbociclib;Sorafenib ë“±ì´ ìˆë‹¤. ì¶”ê°€ ì§ˆë¬¸ì´ í•„ìš”í•œê°€ìš”:ì˜ˆ.
        ì¶”ê°€ ì§ˆë¬¸ : ê° ì‹ ì•½ì˜ ìœ íš¨ì„±(responsive)ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.
        ì¤‘ê°„ ë‹µë³€ : cancer size volume 40% ê°ì†Œì´ë©´ y/n, 60% ê°ì†Œì´ë©´ y/n respectively.
        ì¶”ê°€ ì§ˆë¬¸ : ê° ì‹ ì•½ì˜ ë…ì„±(Drug Toxicity Mortality) ê´€ë ¨ ì •ë³´ë¥¼ ë§í•´ì¤˜.
        ì¤‘ê°„ ë‹µë³€ : 1ë§ˆë¦¬ ì´ìƒ ì‚¬ë§ì´ë©´ o/x respectively.
        ì¶”ê°€ ì§ˆë¬¸ : ì¢…í•© í…Œì´ë¸” í˜•íƒœë¡œ ì •ë¦¬í•´ì¤˜.
        ìµœì¢… ë‹µë³€ :
        | ì‹ ì•½ëª…       | ìœ íš¨ì„±(responsive) | ë…ì„±(Drug Toxicity Mortality) |
        |--------------|--------------------|-------------------------------|
        | Trametinib   | y                  | x                             |
        | Palbociclib  | n                  | o                             |
        | Sorafenib    | y                  | x                             |   
        """
    }
]

def rag_answer_questions(questions, retriever, chunks, embedder):
    results = {}
    for q in questions:
        # ì„ë² ë”©: ì§ˆë¬¸ ì„ë² ë”©
        q_emb = embedder.encode([q], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = retriever.search(q_emb, TOP_K)  # I shape (1, k)
        idxs = I[0].tolist()
        context = "\n\n---\n\n".join([chunks[i] for i in idxs if i < len(chunks)])
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ëª…í™•í•œ í¬ë§· ìš”ì²­
        prompt = FewShotPromptTemplate(
            examples=examples,
            example_prompt=PromptTemplate(
                input_variables=["question", "answer"],
                template="ì§ˆë¬¸: {question}\në‹µë³€: {answer}\n"
            ),
            prefix="ë‹¤ìŒ ë¬¸ì„œ ë°œì·Œë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n"
                   f"==== ë¬¸ì„œ ë°œì·Œ ì‹œì‘ ====\n{context}\n==== ë¬¸ì„œ ë°œì·Œ ë ====\n\n",
            suffix="ì§ˆë¬¸: {question}\nìš”êµ¬ì‚¬í•­:\n"
                   "ìœ íš¨ì„±(responsive)ì´ë€, ì•”ì˜ ë¬´ê²Œ/ë¶€í”¼ê°€ ê°ê° 40%, 60% ê°ì†Œí–ˆìœ¼ë©´ y, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ nìœ¼ë¡œ ë‹µí•  ê²ƒ\n"
                   "ë…ì„±(Drug Toxicity Mortality)ì´ë€, 1ë§ˆë¦¬ ì´ìƒì˜ ì¥ê°€ ì‚¬ë§í–ˆë‹¤ë©´ o, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ x\n"
                   "ì‘ë‹µ:",
            input_variables=["question"]
        ).format(question=q)    
        
        answer, err = call_ollama(prompt)
        if err:
            results[q] = {"answer": None, "error": err, "context_idxs": idxs}
        else:
            results[q] = {"answer": answer, "error": None, "context_idxs": idxs}
    return results

# -----------------------
# Streamlit UI
# -----------------------
st.set_page_config(page_title="PDF â†’ RAG â†’ GPT-OSS (ì‹ ì•½ ì •ë³´ ìë™í™”)", layout="wide")
st.title("ğŸ”¬ PDF â†’ RAG â†’ GPT-OSS : ì‹ ì•½ëª… ìë™ ì¶”ì¶œ")

st.markdown("ì—…ë¡œë“œ ëœ ë…¼ë¬¸(PDF)ì—ì„œ **ì‹ ì•½ëª… / íš¨ëŠ¥(responsive) / ë…ì„±(toxic)** í•­ëª©ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. Ollama(gpt-oss)ì™€ ë¡œì»¬ ì„ë² ë”©+FAISSë¥¼ ì‚¬ìš©í•œ RAG ë°©ì‹ì…ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("ë…¼ë¬¸(PDF) ì—…ë¡œë“œ", type=["pdf"])
run_button = st.button("ë¶„ì„ ì‹œì‘")

if uploaded_file and run_button:
    raw = uploaded_file.read()
    fid = file_hash_bytes(raw)
    st.info(f"íŒŒì¼ í•´ì‹œ: {fid[:12]}... (ìºì‹±ìš©)")
    with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
        text = extract_text_from_pdf_bytes(raw)

    if not text:
        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”í•œ ì´ë¯¸ì§€í˜• PDFì¸ ê²½ìš° OCR í•„ìš”.")
    else:
        st.success(f"ì´ ë¬¸ì ìˆ˜: {len(text)}")
        # ì²­í¬ ë¶„í• 
        chunks = chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)
        st.write(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)} (ì²­í¬ ê¸¸ì´={CHUNK_SIZE}, ê²¹ì¹¨={CHUNK_OVERLAP})")

        # ì„ë² ë”© & FAISS (ìºì‹œ íŒŒì¼ í´ë” ì‚¬ìš©)
        cache_dir = os.path.join(".cache_rag")
        os.makedirs(cache_dir, exist_ok=True)
        index_path = os.path.join(cache_dir, f"{fid}.index")
        chunks_path = os.path.join(cache_dir, f"{fid}_chunks.json")

        embedder = load_embedder()

        if os.path.exists(index_path) and os.path.exists(chunks_path):
            try:
                st.info("ì´ íŒŒì¼ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ìºì‹œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
                idx = faiss.read_index(index_path)
                with open(chunks_path, "r", encoding="utf-8") as f:
                    chunks = json.load(f)
                retriever = idx
            except Exception as e:
                st.warning("ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¸ë±ìŠ¤ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                os.remove(index_path) if os.path.exists(index_path) else None
                # fallthrough to create
                retriever = None
        else:
            retriever = None

        if retriever is None:
            with st.spinner("ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘... (ì•½ê°„ ì‹œê°„ ì†Œìš”)"):
                # ë°°ì¹˜ ì„ë² ë”©
                batch_size = 64
                embeddings = []
                for i in tqdm(range(0, len(chunks), batch_size), desc="Embedding"):
                    batch = chunks[i:i+batch_size]
                    embs = embedder.encode(batch, convert_to_numpy=True)
                    embeddings.append(embs)
                embeddings = np.vstack(embeddings).astype("float32")
                # Build index
                if embeddings.shape[1] != EMBED_DIM:
                    st.warning(f"ì„ë² ë”© ì°¨ì› {embeddings.shape[1]} (ì˜ˆìƒ {EMBED_DIM})ì…ë‹ˆë‹¤. EMBED_DIM ê°’ì„ ë§ì¶°ì£¼ì„¸ìš”.")
                faiss.normalize_L2(embeddings)
                retriever = faiss.IndexFlatIP(embeddings.shape[1])
                retriever.add(embeddings)
                # save index and chunks
                faiss.write_index(retriever, index_path)
                with open(chunks_path, "w", encoding="utf-8") as f:
                    json.dump(chunks, f, ensure_ascii=False)
                st.success("FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ë° ìºì‹œ ì €ì¥ë¨.")

        # ìë™ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        questions = [
            "ë…¼ë¬¸ì—ì„œ ì‹ ì•½ëª… ì¶”ì¶œí•˜ê³  ë‚˜ì„œ, examples ì°¸ê³ í•´ì„œ ëŒ€ë‹µí•´\n" 
            "ì˜ì–´ë¡œ ë‹µë³€í•´ "
        ]

        st.info("ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Ollama(gpt-oss)ì— ì§ˆì˜í•©ë‹ˆë‹¤...")
        results = rag_answer_questions(questions, retriever, chunks, embedder)

        # ê²°ê³¼ í‘œì‹œ
        for q in questions:
            st.markdown("---")
            st.subheader(q)
            entry = results[q]
            if entry["error"]:
                st.error(f"ì§ˆì˜ ì‹¤íŒ¨: {entry['error']}")
                st.write("ê´€ë ¨ ì²­í¬ ì¸ë±ìŠ¤:", entry.get("context_idxs"))
            else:
                st.write(entry["answer"])
                # ê°„ëµí•˜ê²Œ ê´€ë ¨ ì²­í¬ë„ ë³´ì—¬ì£¼ê¸°
                # st.markdown("**ì°¸ì¡°ëœ ë¬¸ì¥(ì²­í¬):**")
                # for idx in entry.get("context_idxs", []):
                    # if idx < len(chunks):
                        # st.write(f"- (idx {idx}) " + chunks[idx][:400].replace("\n", " ") + ("..." if len(chunks[idx]) > 400 else ""))
st.markdown("---")
st.caption("Tip: ì²­í¬ í¬ê¸°, overlap, top_k ê°’ì€ ë¬¸ì„œ íŠ¹ì„±ì— ë”°ë¼ ì¡°ì ˆí•˜ì„¸ìš”.")
